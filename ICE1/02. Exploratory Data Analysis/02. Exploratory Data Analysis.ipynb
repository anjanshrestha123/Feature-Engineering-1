{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll do some exploratory data analysis over our dataset. However, since we don't have our features created yet, we cannot do much at this point. In addition, when we create them, we won't be able to extract many insights because of the nature of text-based features. For this reason, only a shallow analysis will be done at this point.\n",
    "\n",
    "For the plots we have used `seaborn` and `altair`. `altair` is a package which allows us to plot graphics with a simple grammar as we would do in ggplot2 or Tableau. It also provides easy-to-make interactive plots. For further information please visit the project site: https://altair-viz.github.io/.\n",
    "\n",
    "To install it, please type this command in the shell:\n",
    "\n",
    "`! conda install -c conda-forge altair vega_datasets notebook vega`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.9.2\n",
      "  latest version: 4.10.3\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/aashish/opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - altair\n",
      "    - notebook\n",
      "    - vega\n",
      "    - vega_datasets\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    altair-4.1.0               |             py_1         614 KB  conda-forge\n",
      "    certifi-2019.9.11          |           py37_0         147 KB  conda-forge\n",
      "    conda-4.10.3               |   py37hf985489_0         3.0 MB  conda-forge\n",
      "    notebook-6.0.1             |           py37_0         6.0 MB  conda-forge\n",
      "    python_abi-3.7             |          2_cp37m           4 KB  conda-forge\n",
      "    vega-2.6.0                 |           py37_0         1.8 MB  conda-forge\n",
      "    vega_datasets-0.9.0        |     pyhd3deb0d_0         179 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        11.8 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  altair             conda-forge/noarch::altair-4.1.0-py_1\n",
      "  python_abi         conda-forge/osx-64::python_abi-3.7-2_cp37m\n",
      "  vega               conda-forge/osx-64::vega-2.6.0-py37_0\n",
      "  vega_datasets      conda-forge/noarch::vega_datasets-0.9.0-pyhd3deb0d_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  conda               pkgs/main::conda-4.9.2-py37hecd8cb5_0 --> conda-forge::conda-4.10.3-py37hf985489_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi                                         pkgs/main --> conda-forge\n",
      "  notebook                                        pkgs/main --> conda-forge\n",
      "\n",
      "\n",
      "Proceed ([y]/n)? "
     ]
    }
   ],
   "source": [
    "! conda install -c conda-forge altair vega_datasets notebook vega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'altair'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a60fa0867c28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"whitegrid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0maltair\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"notebook\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'altair'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "import altair as alt\n",
    "alt.renderers.enable(\"notebook\")\n",
    "\n",
    "# Code for hiding seaborn warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = \"/home/lnc/0. Latest News Classifier/01. Dataset Creation/\"\n",
    "df_path2 = df_path + 'News_dataset.csv'\n",
    "df = pd.read_csv(df_path2, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of articles in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bars = alt.Chart(df).mark_bar(size=50).encode(\n",
    "    x=alt.X(\"Category\"),\n",
    "    y=alt.Y(\"count():Q\", axis=alt.Axis(title='Number of articles')),\n",
    "    tooltip=[alt.Tooltip('count()', title='Number of articles'), 'Category'],\n",
    "    color='Category'\n",
    "\n",
    ")\n",
    "\n",
    "text = bars.mark_text(\n",
    "    align='center',\n",
    "    baseline='bottom',\n",
    ").encode(\n",
    "    text='count()'\n",
    ")\n",
    "\n",
    "(bars + text).interactive().properties(\n",
    "    height=300, \n",
    "    width=700,\n",
    "    title = \"Number of articles in each category\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### % of articles in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id'] = 1\n",
    "df2 = pd.DataFrame(df.groupby('Category').count()['id']).reset_index()\n",
    "\n",
    "bars = alt.Chart(df2).mark_bar(size=50).encode(\n",
    "    x=alt.X('Category'),\n",
    "    y=alt.Y('PercentOfTotal:Q', axis=alt.Axis(format='.0%', title='% of Articles')),\n",
    "    color='Category'\n",
    ").transform_window(\n",
    "    TotalArticles='sum(id)',\n",
    "    frame=[None, None]\n",
    ").transform_calculate(\n",
    "    PercentOfTotal=\"datum.id / datum.TotalArticles\"\n",
    ")\n",
    "\n",
    "text = bars.mark_text(\n",
    "    align='center',\n",
    "    baseline='bottom',\n",
    "    #dx=5  # Nudges text to right so it doesn't appear on top of the bar\n",
    ").encode(\n",
    "    text=alt.Text('PercentOfTotal:Q', format='.1%')\n",
    ")\n",
    "\n",
    "(bars + text).interactive().properties(\n",
    "    height=300, \n",
    "    width=700,\n",
    "    title = \"% of articles in each category\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are approximately balanced. We'll first try to train the models without oversampling/undersampling. If we see some bias in the model, we'll use these techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News length by category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of news length field. Although there are special characters in the text (``\\r, \\n``), it will be useful as an approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['News_length'] = df['Content'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12.8,6))\n",
    "sns.distplot(df['News_length']).set_title('News length distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['News_length'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove from the 95% percentile onwards to better appreciate the histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_95 = df['News_length'].quantile(0.95)\n",
    "df_95 = df[df['News_length'] < quantile_95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12.8,6))\n",
    "sns.distplot(df_95['News_length']).set_title('News length distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the number of news articles with more than 10,000 characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_more10k = df[df['News_length'] > 10000]\n",
    "len(df_more10k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_more10k['Content'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's just a large news article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot a boxplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12.8,6))\n",
    "sns.boxplot(data=df, x='Category', y='News_length', width=.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's remove the larger documents for better comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12.8,6))\n",
    "sns.boxplot(data=df_95, x='Category', y='News_length');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, although the length distribution is different for every category, the difference is not too big. If we had way too different lengths between categories we would have a problem since the feature creation process may take into account counts of words. However, when creating the features with TF-IDF scoring, we will normalize the features just to avoid this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we cannot do further Exploratory Data Analysis. We'll turn onto the **Feature Engineering** section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll save the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('News_dataset.pickle', 'wb') as output:\n",
    "    pickle.dump(df, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
